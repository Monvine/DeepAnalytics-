import requests
import json
import time
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from datetime import datetime, timedelta
from tqdm import tqdm
from collections import defaultdict
import jieba
import jieba.analyse
import re
from sqlalchemy import create_engine, text, Column, String, Integer, DateTime, Text, DECIMAL
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
import pymysql
from typing import Optional, Dict, Any, List
import os
import base64
from io import BytesIO
from fastapi import FastAPI, HTTPException, BackgroundTasks, Depends, Request, Header
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel
from apscheduler.schedulers.background import BackgroundScheduler
import logging
from ml_models import MLService
from auth import AuthService
from ai_service import AIService
from report_service import ReportService

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def get_chinese_font():
    """èŽ·å–å¯ç”¨çš„ä¸­æ–‡å­—ä½“è·¯å¾„"""
    import os
    font_paths = [
        'C:/Windows/Fonts/simhei.ttf',
        'C:/Windows/Fonts/simsun.ttc',
        'C:/Windows/Fonts/msyh.ttc',
        'C:/Windows/Fonts/simkai.ttf',
        '/System/Library/Fonts/PingFang.ttc',
        '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf',
    ]

    for font_path in font_paths:
        if os.path.exists(font_path):
            return font_path

    return None

jieba.initialize()

MYSQL_CONFIG = {
    'host': 'localhost',
    'user': 'root',
    'password': 'htlovewql1314',
    'database': 'bilibili_analysis',
    'charset': 'utf8mb4'
}

engine = create_engine(
    f"mysql+pymysql://{MYSQL_CONFIG['user']}:{MYSQL_CONFIG['password']}@"
    f"{MYSQL_CONFIG['host']}/{MYSQL_CONFIG['database']}?charset={MYSQL_CONFIG['charset']}"
)

from config import DEFAULT_COOKIE, DEEPSEEK_API_KEY, validate_config

class CookieRequest(BaseModel):
    cookie: str

class AnalysisResponse(BaseModel):
    total_videos: int
    avg_views: int
    avg_interaction_rate: float
    top_tags: List[List]
    best_publish_hours: Optional[List[int]] = None

class UserInfoResponse(BaseModel):
    name: str
    mid: int
    level: int
    following: int
    follower: int

class UserRegister(BaseModel):
    username: str
    email: str
    password: str

class UserLogin(BaseModel):
    username: str
    password: str

class BilibiliCookieUpdate(BaseModel):
    cookie: str

class AIQueryRequest(BaseModel):
    query: str
    conversation_history: Optional[List[Dict[str, str]]] = None

class AIQueryResponse(BaseModel):
    success: bool
    response: str
    intent: Optional[Dict[str, Any]] = None
    data_context_available: bool = False
    timestamp: str
    model: str = "deepseek-v3"
    error: Optional[str] = None

class DailyReportRequest(BaseModel):
    target_date: Optional[str] = None

class WeeklyReportRequest(BaseModel):
    week_start: Optional[str] = None

app = FastAPI(title="Bç«™æ•°æ®åˆ†æžç³»ç»Ÿ", version="1.0.0")

ml_service = MLService()

auth_service = AuthService(engine)

ai_service = AIService(
    api_key=DEEPSEEK_API_KEY,
    engine=engine
)

report_service = ReportService(engine=engine)

security = HTTPBearer(auto_error=False)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class BiliBiliAnalyticsSystem:
    def __init__(self):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Referer": "https://www.bilibili.com/",
            "Origin": "https://www.bilibili.com"
        }
        self.base_url = "https://api.bilibili.com"
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        self.session.encoding = 'utf-8'
        self._init_db()

    def _init_db(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨"""
        try:
            with engine.begin() as conn:
                conn.execute(text("""
                CREATE TABLE IF NOT EXISTS videos (
                    bvid VARCHAR(20) PRIMARY KEY,
                    title TEXT,
                    aid VARCHAR(20),
                    author VARCHAR(100),
                    mid VARCHAR(20),
                    view INT,
                    danmaku INT,
                    reply INT,
                    favorite INT,
                    coin INT,
                    share INT,
                    `like` INT,
                    duration INT,
                    pubdate DATETIME,
                    tid INT,
                    tname VARCHAR(50),
                    copyright TINYINT,
                    tags TEXT,
                    `desc` TEXT,
                    ctime DATETIME,
                    collected_at DATETIME,
                    INDEX idx_mid (mid),
                    INDEX idx_pubdate (pubdate),
                    INDEX idx_tid (tid)
                )
                """))

                # åˆ›å»ºç”¨æˆ·æ•°æ®è¡¨
                conn.execute(text("""
                CREATE TABLE IF NOT EXISTS user_data (
                    id INT AUTO_INCREMENT PRIMARY KEY,
                    user_mid VARCHAR(20),
                    data_type VARCHAR(50),
                    data_content JSON,
                    created_at DATETIME,
                    INDEX idx_user_mid (user_mid),
                    INDEX idx_data_type (data_type)
                )
                """))

            logger.info("æ•°æ®åº“è¡¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise

    def crawl_popular_videos(self, pages=5):
        """çˆ¬å–çƒ­é—¨è§†é¢‘ï¼ˆç§»é™¤UPä¸»ä¿¡æ¯éƒ¨åˆ†ï¼‰"""
        for page in tqdm(range(1, pages + 1), desc="çˆ¬å–çƒ­é—¨è§†é¢‘"):
            try:
                url = f"{self.base_url}/x/web-interface/popular?pn={page}"
                response = self.session.get(url, timeout=10)
                response.encoding = 'utf-8'
                response.raise_for_status()
                data = response.json()

                if data.get("code") == 0:
                    for item in data["data"]["list"]:
                        self._process_video_item(item)

                time.sleep(1.5)

            except requests.exceptions.RequestException as e:
                logger.error(f"ç¬¬{page}é¡µè¯·æ±‚å¤±è´¥: {str(e)}")
                time.sleep(5)
            except Exception as e:
                logger.error(f"ç¬¬{page}é¡µå¤„ç†å¤±è´¥: {str(e)}")
                time.sleep(3)

    def get_video_details(self, bvid: str) -> Optional[Dict[str, Any]]:
        """èŽ·å–è§†é¢‘è¯¦æƒ…"""
        try:
            detail_url = f"{self.base_url}/x/web-interface/view?bvid={bvid}"
            detail_res = self.session.get(detail_url, timeout=10)
            detail_res.encoding = 'utf-8'
            detail_res.raise_for_status()
            detail_data = detail_res.json()

            if detail_data.get("code") != 0:
                return None

            detail = detail_data["data"]

            tags = []
            if "aid" in detail:
                tag_url = f"{self.base_url}/x/tag/archive/tags?aid={detail['aid']}"
                tag_res = self.session.get(tag_url, timeout=10)
                tag_res.encoding = 'utf-8'
                if tag_res.status_code == 200:
                    tag_data = tag_res.json()
                    if tag_data.get("code") == 0:
                        tags = [tag["tag_name"] for tag in tag_data.get("data", [])]

            if not tags and "dynamic" in detail:
                dynamic_text = str(detail.get("dynamic", ""))
                tags = re.findall(r'#([^#\s]+)#', dynamic_text)

            if not tags and "tname" in detail:
                tags = [detail["tname"]]

            detail["processed_tags"] = tags
            return detail

        except Exception as e:
            logger.error(f"èŽ·å–è§†é¢‘{bvid}è¯¦æƒ…å¤±è´¥: {str(e)}")
            return None

    def _process_video_item(self, item: Dict[str, Any]):
        """å¤„ç†è§†é¢‘æ•°æ®å¹¶å­˜å…¥MySQLï¼ˆç§»é™¤UPä¸»ä¿¡æ¯å¤„ç†ï¼‰"""
        try:
            detail = self.get_video_details(item["bvid"])
            if not detail:
                return

            video_data = {
                "bvid": detail.get("bvid", ""),
                "title": detail.get("title", ""),
                "aid": str(detail.get("aid", "")),
                "author": detail.get("owner", {}).get("name", ""),
                "mid": str(detail.get("owner", {}).get("mid", "")),
                "view": detail.get("stat", {}).get("view", 0),
                "danmaku": detail.get("stat", {}).get("danmaku", 0),
                "reply": detail.get("stat", {}).get("reply", 0),
                "favorite": detail.get("stat", {}).get("favorite", 0),
                "coin": detail.get("stat", {}).get("coin", 0),
                "share": detail.get("stat", {}).get("share", 0),
                "like": detail.get("stat", {}).get("like", 0),
                "duration": detail.get("duration", 0),
                "pubdate": datetime.fromtimestamp(detail.get("pubdate", 0)) if detail.get("pubdate") else None,
                "tid": detail.get("tid", 0),
                "tname": detail.get("tname", ""),
                "copyright": detail.get("copyright", 0),
                "tags": ",".join(detail.get("processed_tags", [])),
                "desc": detail.get("desc", ""),
                "ctime": datetime.fromtimestamp(detail.get("ctime", 0)) if detail.get("ctime") else None,
                "collected_at": datetime.now()
            }

            with engine.begin() as conn:
                conn.execute(text("""
                INSERT INTO videos (
                    bvid, title, aid, author, mid, view, danmaku, reply, 
                    favorite, coin, share, `like`, duration, pubdate, tid, 
                    tname, copyright, tags, `desc`, ctime, collected_at
                ) VALUES (
                    :bvid, :title, :aid, :author, :mid, :view, 
                    :danmaku, :reply, :favorite, :coin, :share, 
                    :like, :duration, :pubdate, :tid, :tname, 
                    :copyright, :tags, :desc, :ctime, :collected_at
                )
                ON DUPLICATE KEY UPDATE 
                    title=VALUES(title), view=VALUES(view), danmaku=VALUES(danmaku),
                    reply=VALUES(reply), favorite=VALUES(favorite), coin=VALUES(coin),
                    share=VALUES(share), `like`=VALUES(`like`), tags=VALUES(tags)
                """), video_data)

            logger.info(f"æˆåŠŸå¤„ç†è§†é¢‘: {item['bvid']}")

        except Exception as e:
            logger.error(f"å¤„ç†è§†é¢‘{item['bvid']}å¤±è´¥: {str(e)}")

    def load_data_to_dataframe(self) -> pd.DataFrame:
        """ä»ŽMySQLåŠ è½½æ•°æ®åˆ°DataFrame"""
        try:
            with engine.connect() as conn:
                df = pd.read_sql("""
                SELECT 
                    bvid, title, author, view, danmaku, reply, 
                    favorite, coin, share, `like`, duration, 
                    pubdate, tname, tags, `desc`
                FROM videos
                ORDER BY collected_at DESC
                """, conn)

            if df.empty:
                return pd.DataFrame()

            if 'pubdate' in df:
                df['pubdate'] = pd.to_datetime(df['pubdate'])

            interaction_cols = ['danmaku', 'reply', 'favorite', 'coin', 'share', 'like']
            for col in interaction_cols:
                if col not in df:
                    df[col] = 0

            df['interaction_rate'] = (
                df['danmaku'] + df['reply'] + df['favorite'] +
                df['coin'] + df['share'] + df['like']
            ) / df['view'].clip(lower=1)

            if 'pubdate' in df:
                df['pub_hour'] = df['pubdate'].dt.hour

            return df

        except Exception as e:
            logger.error(f"åŠ è½½æ•°æ®å¤±è´¥: {str(e)}")
            return pd.DataFrame()

    def analyze_and_visualize(self, df: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """æ•°æ®åˆ†æžå’Œå¯è§†åŒ–"""
        if df.empty:
            return None

        try:
            plt.figure(figsize=(18, 15))
            plt.rcParams['font.sans-serif'] = ['SimHei']
            plt.rcParams['axes.unicode_minus'] = False
            plt.style.use('default')
            plt.rcParams['text.color'] = 'black'
            plt.rcParams['axes.labelcolor'] = 'black'
            plt.rcParams['xtick.color'] = 'black'
            plt.rcParams['ytick.color'] = 'black'
            plt.rcParams['axes.titlecolor'] = 'black'
            plt.rcParams['figure.facecolor'] = 'white'
            plt.rcParams['axes.facecolor'] = 'white'

            plt.subplot(3, 2, 1)
            sns.histplot(np.log10(df['view'] + 1), bins=30, kde=True, color='skyblue', alpha=0.7)
            plt.title('çƒ­é—¨è§†é¢‘æ’­æ”¾é‡åˆ†å¸ƒ', color='black', fontsize=12, fontweight='bold')
            plt.xlabel('æ’­æ”¾é‡(log10)', color='black')
            plt.ylabel('è§†é¢‘æ•°é‡', color='black')

            plt.subplot(3, 2, 2)
            all_tags = ' '.join(df['tags'].fillna('').astype(str))

            font_path = get_chinese_font()
            wordcloud_config = {
                'background_color': 'white',
                'width': 800, 
                'height': 600,
                'max_words': 100,
                'collocations': False
            }
            if font_path:
                wordcloud_config['font_path'] = font_path

            if not all_tags.strip():
                all_titles = ' '.join(df['title'].dropna().astype(str))
                word_list = jieba.analyse.extract_tags(all_titles, topK=100, withWeight=True)
                word_dict = {word: weight for word, weight in word_list}
                wordcloud = WordCloud(**wordcloud_config).generate_from_frequencies(word_dict)
            else:
                wordcloud = WordCloud(**wordcloud_config).generate(all_tags)

            plt.imshow(wordcloud, interpolation='bilinear')
            plt.axis('off')
            plt.title('çƒ­é—¨è§†é¢‘æ ‡ç­¾è¯äº‘', color='black', fontsize=12, fontweight='bold')

            plt.subplot(3, 2, 3)
            if 'tname' in df and not df['tname'].isna().all():
                category_stats = df['tname'].value_counts().head(10)
                plt.pie(category_stats.values, labels=category_stats.index, autopct='%1.1f%%', textprops={'color': 'black'})
                plt.title('çƒ­é—¨è§†é¢‘åˆ†åŒºåˆ†å¸ƒ', color='black', fontsize=12, fontweight='bold')
            else:
                plt.text(0.5, 0.5, 'æš‚æ— åˆ†åŒºæ•°æ®', ha='center', va='center', transform=plt.gca().transAxes, color='black')
                plt.title('è§†é¢‘åˆ†åŒºåˆ†å¸ƒ', color='black', fontsize=12, fontweight='bold')

            plt.subplot(3, 2, 4)
            interaction_df = df[['danmaku', 'reply', 'favorite', 'coin', 'share', 'like']].corr()
            sns.heatmap(interaction_df, annot=True, cmap='coolwarm', center=0, fmt=".2f", 
                       annot_kws={'color': 'black'}, cbar_kws={'label': 'ç›¸å…³ç³»æ•°'})
            plt.title('çƒ­é—¨è§†é¢‘äº’åŠ¨è¡Œä¸ºç›¸å…³æ€§', color='black', fontsize=12, fontweight='bold')

            plt.subplot(3, 2, 5)
            if 'pub_hour' in df and not df['pub_hour'].isna().all():
                hour_stats = df['pub_hour'].value_counts().sort_index()
                plt.bar(hour_stats.index, hour_stats.values, color='skyblue', alpha=0.7)
                plt.title('çƒ­é—¨è§†é¢‘å‘å¸ƒæ—¶é—´åˆ†å¸ƒ', color='black', fontsize=12, fontweight='bold')
                plt.xlabel('å‘å¸ƒæ—¶é—´(å°æ—¶)', color='black')
                plt.ylabel('è§†é¢‘æ•°é‡', color='black')
                plt.xticks(range(0, 24, 2), color='black')
                plt.yticks(color='black')
                plt.grid(True, alpha=0.3)
            else:
                plt.text(0.5, 0.5, 'æš‚æ— æ—¶é—´æ•°æ®', ha='center', va='center', transform=plt.gca().transAxes, color='black')
                plt.title('è§†é¢‘å‘å¸ƒæ—¶é—´åˆ†å¸ƒ', color='black', fontsize=12, fontweight='bold')

            plt.subplot(3, 2, 6)
            corr_cols = ['view', 'danmaku', 'reply', 'favorite', 'coin', 'share', 'like', 'interaction_rate']
            available_cols = [col for col in corr_cols if col in df]
            corr_df = df[available_cols].corr()

            sns.heatmap(corr_df[['view']].sort_values('view', ascending=False),
                        annot=True, cmap='viridis', vmin=-1, vmax=1, fmt=".2f",
                        annot_kws={'color': 'white'}, cbar_kws={'label': 'ç›¸å…³ç³»æ•°'})
            plt.title('çƒ­é—¨è§†é¢‘æ’­æ”¾é‡ä¸Žäº’åŠ¨æŒ‡æ ‡ç›¸å…³æ€§', color='black', fontsize=12, fontweight='bold')

            plt.tight_layout()

            img_buffer = BytesIO()
            plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')
            img_buffer.seek(0)

            plt.savefig('static/bilibili_analysis.png', dpi=300, bbox_inches='tight')
            plt.close()

            analysis_results = {
                "total_videos": len(df),
                "avg_views": int(df['view'].mean()),
                "avg_interaction_rate": float(df['interaction_rate'].mean()),
                "top_tags": self._extract_top_tags(df),
            }

            if 'pub_hour' in df:
                hour_stats = df.groupby('pub_hour')['view'].agg(['mean', 'count'])
                hour_stats = hour_stats[hour_stats['count'] > 5]
                analysis_results["best_publish_hours"] = hour_stats.sort_values('mean', ascending=False).head(3).index.tolist()

            return analysis_results

        except Exception as e:
            logger.error(f"æ•°æ®åˆ†æžå¤±è´¥: {str(e)}")
            return None

    def _extract_top_tags(self, df: pd.DataFrame, n: int = 10) -> list:
        """æå–çƒ­é—¨æ ‡ç­¾"""
        tag_counter = defaultdict(int)
        for tags in df['tags'].dropna().astype(str):
            for tag in tags.split(','):
                if tag.strip():
                    tag_counter[tag.strip()] += 1
        return [[tag, count] for tag, count in sorted(tag_counter.items(), key=lambda x: x[1], reverse=True)[:n]]


class BiliBiliUserCrawler:
    def __init__(self, cookie: str = DEFAULT_COOKIE):
        self.cookie = self._clean_cookie(cookie.strip() if cookie else DEFAULT_COOKIE)
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:138.0) Gecko/20100101 Firefox/138.0',
            'Accept': '*/*',
            'Accept-Language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2',
            'Accept-Encoding': 'gzip, deflate, br, zstd',
            'Referer': 'https://www.bilibili.com/',
            'Origin': 'https://www.bilibili.com',
            'Connection': 'keep-alive',
            'Cookie': self.cookie,
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-site',
            'Priority': 'u=4',
            'TE': 'trailers'
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        self.session.encoding = 'utf-8'

    def _clean_cookie(self, cookie: str) -> str:
        """æ¸…ç†Cookieä¸­çš„ç‰¹æ®Šå­—ç¬¦"""
        if not cookie:
            return DEFAULT_COOKIE

        import re
        cleaned = re.sub(r'[^\x20-\x7E]', '', cookie)
        cleaned = ' '.join(cleaned.split())

        return cleaned if cleaned else DEFAULT_COOKIE

    def check_cookie_validity(self) -> bool:
        """æ£€æŸ¥Cookieæ˜¯å¦æœ‰æ•ˆ"""
        try:
            user_info = self.get_user_info()
            return user_info is not None
        except:
            return False

    def get_user_info(self):
        """èŽ·å–Bç«™ä¸ªäººä¿¡æ¯"""
        url = 'https://api.bilibili.com/x/space/myinfo'
        try:
            response = self.session.get(url, timeout=10)
            response.encoding = 'utf-8'

            if response.status_code == 200:
                data = response.json()
                if data.get('code') == 0:
                    return data['data']
                elif data.get('code') == -101:
                    logger.warning("Cookieå·²è¿‡æœŸæˆ–è´¦å·æœªç™»å½•")
                    return None
                else:
                    logger.error(f"èŽ·å–ä¸ªäººä¿¡æ¯å¤±è´¥: {data.get('message')} (code: {data.get('code')})")
            else:
                logger.error(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
        except requests.exceptions.JSONDecodeError as e:
            logger.error(f"JSONè§£æžå¤±è´¥: {str(e)}")
        except Exception as e:
            logger.error(f"èŽ·å–ä¸ªäººä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
        return None

    def get_watch_history(self, max_pages=5):
        """èŽ·å–è§‚çœ‹åŽ†å²è®°å½•"""
        history = []
        url = 'https://api.bilibili.com/x/web-interface/history/cursor'
        params = {
            'view_at': 0,
            'business': 'archive'
        }

        for _ in range(max_pages):
            try:
                response = self.session.get(url, params=params, timeout=10)
                response.encoding = 'utf-8'

                if response.status_code != 200:
                    break

                data = response.json()
                if data.get('code') != 0:
                    break

                history_data = data.get('data', {})
                history.extend(history_data.get('list', []))

                if not history_data.get('cursor', {}).get('max'):
                    break

                params['view_at'] = history_data['cursor']['max']
                time.sleep(1)

            except Exception as e:
                logger.error(f"èŽ·å–åŽ†å²è®°å½•æ—¶å‡ºé”™: {str(e)}")
                break

        return history

    def get_favorites(self, mid):
        """èŽ·å–æ”¶è—å†…å®¹"""
        favorites = []
        folder_url = 'https://api.bilibili.com/x/v3/fav/folder/created/list-all'
        folder_params = {'up_mid': mid}

        try:
            folder_response = self.session.get(folder_url, params=folder_params, timeout=10)
            folder_response.encoding = 'utf-8'

            if folder_response.status_code == 200:
                folder_data = folder_response.json()
                if folder_data.get('code') == 0:
                    folders = folder_data.get('data', {}).get('list', [])

                    for folder in folders:
                        media_url = 'https://api.bilibili.com/x/v3/fav/resource/list'
                        media_params = {
                            'media_id': folder['id'],
                            'ps': 20,
                            'pn': 1
                        }

                        media_response = self.session.get(media_url, params=media_params, timeout=10)
                        media_response.encoding = 'utf-8'

                        if media_response.status_code == 200:
                            media_data = media_response.json()
                            if media_data.get('code') == 0:
                                folder['resources'] = media_data.get('data', {}).get('medias', [])
                                favorites.append(folder)
                        time.sleep(0.5)
        except Exception as e:
            logger.error(f"èŽ·å–æ”¶è—å†…å®¹æ—¶å‡ºé”™: {str(e)}")

        return favorites

    def save_user_data(self, user_mid: str, data_type: str, data_content: dict):
        """ä¿å­˜ç”¨æˆ·æ•°æ®åˆ°æ•°æ®åº“"""
        try:
            with engine.begin() as conn:
                conn.execute(text("""
                INSERT INTO user_data (user_mid, data_type, data_content, created_at)
                VALUES (:user_mid, :data_type, :data_content, :created_at)
                """), {
                    'user_mid': user_mid,
                    'data_type': data_type,
                    'data_content': json.dumps(data_content, ensure_ascii=False),
                    'created_at': datetime.now()
                })
        except Exception as e:
            logger.error(f"ä¿å­˜ç”¨æˆ·æ•°æ®å¤±è´¥: {str(e)}")


# å…¨å±€å®žä¾‹
analytics_system = BiliBiliAnalyticsSystem()
scheduler = BackgroundScheduler()

# åˆ›å»ºé™æ€æ–‡ä»¶ç›®å½•
os.makedirs('static', exist_ok=True)

def scheduled_crawl():
    """å®šæ—¶çˆ¬å–ä»»åŠ¡"""
    logger.info("å¼€å§‹å®šæ—¶çˆ¬å–çƒ­é—¨è§†é¢‘...")
    try:
        analytics_system.crawl_popular_videos(pages=3)
        logger.info("å®šæ—¶çˆ¬å–å®Œæˆ")
    except Exception as e:
        logger.error(f"å®šæ—¶çˆ¬å–å¤±è´¥: {str(e)}")

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶çš„åˆå§‹åŒ–"""
    logger.info("ðŸš€ Bç«™æ•°æ®åˆ†æžç³»ç»Ÿå¯åŠ¨ä¸­...")

    if not validate_config():
        logger.warning("âš ï¸  éƒ¨åˆ†é…ç½®ç¼ºå¤±ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½æ— æ³•æ­£å¸¸ä½¿ç”¨")

    scheduler.add_job(
        scheduled_crawl,
        'interval',
        hours=2,
        id='crawl_popular_videos'
    )
    scheduler.start()
    logger.info("âœ… åº”ç”¨å¯åŠ¨å®Œæˆï¼Œå®šæ—¶ä»»åŠ¡å·²å¯åŠ¨")

@app.on_event("shutdown")
async def shutdown_event():
    """åº”ç”¨å…³é—­æ—¶çš„æ¸…ç†"""
    scheduler.shutdown()

async def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """èŽ·å–å½“å‰ç”¨æˆ·"""
    if not credentials:
        return None

    user = auth_service.get_user_by_token(credentials.credentials)
    return user

async def require_auth(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """éœ€è¦è®¤è¯çš„ä¸­é—´ä»¶"""
    if not credentials:
        raise HTTPException(status_code=401, detail="éœ€è¦ç™»å½•")

    user = auth_service.get_user_by_token(credentials.credentials)
    if not user:
        raise HTTPException(status_code=401, detail="ç™»å½•å·²è¿‡æœŸï¼Œè¯·é‡æ–°ç™»å½•")

    return user

@app.get("/")
async def root():
    return {"message": "Bç«™æ•°æ®åˆ†æžç³»ç»ŸAPI"}


@app.post("/api/auth/register")
async def register(user_data: UserRegister):
    """ç”¨æˆ·æ³¨å†Œ"""
    result = auth_service.register_user(
        username=user_data.username,
        email=user_data.email,
        password=user_data.password
    )

    if result['success']:
        return {
            "message": "æ³¨å†ŒæˆåŠŸ",
            "user": {
                "user_id": result['user_id'],
                "username": result['username'],
                "email": result['email']
            },
            "token": result['token']
        }
    else:
        raise HTTPException(status_code=400, detail=result['error'])

@app.post("/api/auth/login")
async def login(user_data: UserLogin):
    """ç”¨æˆ·ç™»å½•"""
    result = auth_service.login_user(
        username=user_data.username,
        password=user_data.password
    )

    if result['success']:
        return {
            "message": "ç™»å½•æˆåŠŸ",
            "user": {
                "user_id": result['user_id'],
                "username": result['username'],
                "email": result['email'],
                "bilibili_mid": result['bilibili_mid'],
                "bilibili_name": result['bilibili_name']
            },
            "token": result['token']
        }
    else:
        raise HTTPException(status_code=401, detail=result['error'])

@app.post("/api/auth/logout")
async def logout(current_user: dict = Depends(require_auth), credentials: HTTPAuthorizationCredentials = Depends(security)):
    """ç”¨æˆ·ç™»å‡º"""
    success = auth_service.logout_user(credentials.credentials)
    if success:
        return {"message": "ç™»å‡ºæˆåŠŸ"}
    else:
        raise HTTPException(status_code=500, detail="ç™»å‡ºå¤±è´¥")

@app.get("/api/auth/me")
async def get_current_user_info(current_user: dict = Depends(require_auth)):
    """èŽ·å–å½“å‰ç”¨æˆ·ä¿¡æ¯"""
    return {
        "user": current_user
    }

@app.post("/api/auth/update-bilibili")
async def update_bilibili_cookie(
    cookie_data: BilibiliCookieUpdate,
    current_user: dict = Depends(require_auth)
):
    """æ›´æ–°ç”¨æˆ·çš„Bç«™Cookie"""
    try:
        crawler = BiliBiliUserCrawler(cookie_data.cookie)

        if not crawler.check_cookie_validity():
            raise HTTPException(status_code=400, detail="Cookieæ— æ•ˆæˆ–å·²è¿‡æœŸ")

        user_info = crawler.get_user_info()
        if not user_info:
            raise HTTPException(status_code=400, detail="æ— æ³•èŽ·å–Bç«™ç”¨æˆ·ä¿¡æ¯")

        success = auth_service.update_bilibili_info(
            user_id=current_user['user_id'],
            cookie=cookie_data.cookie,
            mid=str(user_info['mid']),
            name=user_info['name']
        )

        if not success:
            raise HTTPException(status_code=500, detail="æ›´æ–°å¤±è´¥")

        await sync_user_bilibili_data(current_user['user_id'], cookie_data.cookie)

        return {
            "message": "Bç«™ä¿¡æ¯æ›´æ–°æˆåŠŸ",
            "bilibili_info": {
                "mid": user_info['mid'],
                "name": user_info['name'],
                "level": user_info.get('level', 0),
                "following": user_info.get('following', 0),
                "follower": user_info.get('follower', 0)
            }
        }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ›´æ–°å¤±è´¥: {str(e)}")

async def sync_user_bilibili_data(user_id: int, cookie: str):
    """åŒæ­¥ç”¨æˆ·çš„Bç«™æ•°æ®"""
    try:
        crawler = BiliBiliUserCrawler(cookie)

        user_info = crawler.get_user_info()
        if user_info:
            with engine.begin() as conn:
                conn.execute(text("""
                INSERT INTO user_data (user_mid, data_type, data_content, created_at)
                VALUES (:user_mid, :data_type, :data_content, :created_at)
                ON DUPLICATE KEY UPDATE 
                data_content = VALUES(data_content), created_at = VALUES(created_at)
                """), {
                    'user_mid': str(user_id),  # ä½¿ç”¨ç³»ç»Ÿç”¨æˆ·ID
                    'data_type': 'user_info',
                    'data_content': json.dumps(user_info, ensure_ascii=False),
                    'created_at': datetime.now()
                })
        
        # èŽ·å–è§‚çœ‹åŽ†å²
        watch_history = crawler.get_watch_history(max_pages=10)  # èŽ·å–æ›´å¤šé¡µé¢
        if watch_history:
            with engine.begin() as conn:
                conn.execute(text("""
                INSERT INTO user_data (user_mid, data_type, data_content, created_at)
                VALUES (:user_mid, :data_type, :data_content, :created_at)
                ON DUPLICATE KEY UPDATE 
                data_content = VALUES(data_content), created_at = VALUES(created_at)
                """), {
                    'user_mid': str(user_id),
                    'data_type': 'watch_history',
                    'data_content': json.dumps(watch_history, ensure_ascii=False),
                    'created_at': datetime.now()
                })
        
        # èŽ·å–æ”¶è—
        if user_info:
            favorites = crawler.get_favorites(user_info['mid'])
            if favorites:
                with engine.begin() as conn:
                    conn.execute(text("""
                    INSERT INTO user_data (user_mid, data_type, data_content, created_at)
                    VALUES (:user_mid, :data_type, :data_content, :created_at)
                    ON DUPLICATE KEY UPDATE 
                    data_content = VALUES(data_content), created_at = VALUES(created_at)
                    """), {
                        'user_mid': str(user_id),
                        'data_type': 'favorites',
                        'data_content': json.dumps(favorites, ensure_ascii=False),
                        'created_at': datetime.now()
                    })
        
        logger.info(f"ç”¨æˆ· {user_id} çš„Bç«™æ•°æ®åŒæ­¥å®Œæˆ")
        
    except Exception as e:
        logger.error(f"åŒæ­¥ç”¨æˆ· {user_id} çš„Bç«™æ•°æ®å¤±è´¥: {str(e)}")

@app.post("/api/crawl/popular")
async def crawl_popular(background_tasks: BackgroundTasks):
    """æ‰‹åŠ¨è§¦å‘çƒ­é—¨è§†é¢‘çˆ¬å–"""
    background_tasks.add_task(analytics_system.crawl_popular_videos, 5)
    return {"message": "çƒ­é—¨è§†é¢‘çˆ¬å–ä»»åŠ¡å·²å¯åŠ¨"}

@app.get("/api/analysis/videos")
async def get_video_analysis():
    """èŽ·å–è§†é¢‘åˆ†æžç»“æžœ"""
    try:
        df = analytics_system.load_data_to_dataframe()
        if df.empty:
            raise HTTPException(status_code=404, detail="æš‚æ— æ•°æ®")

        results = analytics_system.analyze_and_visualize(df)
        if not results:
            raise HTTPException(status_code=500, detail="åˆ†æžå¤±è´¥")

        return results
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/analysis/chart")
async def get_analysis_chart():
    """èŽ·å–åˆ†æžå›¾è¡¨"""
    chart_path = "static/bilibili_analysis.png"
    if os.path.exists(chart_path):
        return FileResponse(chart_path)
    else:
        raise HTTPException(status_code=404, detail="å›¾è¡¨æ–‡ä»¶ä¸å­˜åœ¨")

@app.get("/api/videos")
async def get_videos(page: int = 1, page_size: int = 10, limit: int = None):
    """èŽ·å–è§†é¢‘åˆ—è¡¨ï¼ˆæ”¯æŒåˆ†é¡µï¼‰"""
    try:
        with engine.connect() as conn:
            if limit is not None:
                df = pd.read_sql(f"""
                SELECT bvid, title, author, view, danmaku, reply, 
                       favorite, coin, share, `like`, pubdate, tname
                FROM videos 
                ORDER BY collected_at DESC 
                LIMIT {limit}
                """, conn)
                return df.to_dict('records')
            
            # æ–°çš„åˆ†é¡µé€»è¾‘
            offset = (page - 1) * page_size
            
            # èŽ·å–æ€»æ•°
            total_count_result = conn.execute(text("SELECT COUNT(*) as count FROM videos"))
            total_count = total_count_result.fetchone()[0]
            
            # èŽ·å–åˆ†é¡µæ•°æ®
            df = pd.read_sql(f"""
            SELECT bvid, title, author, view, danmaku, reply, 
                   favorite, coin, share, `like`, pubdate, tname
            FROM videos 
            ORDER BY collected_at DESC 
            LIMIT {page_size} OFFSET {offset}
            """, conn)
            
            return {
                "data": df.to_dict('records'),
                "pagination": {
                    "current": page,
                    "pageSize": page_size,
                    "total": total_count,
                    "totalPages": (total_count + page_size - 1) // page_size
                }
            }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/user/info")
async def get_user_info_with_cookie(cookie_req: CookieRequest = None):
    """èŽ·å–ç”¨æˆ·ä¿¡æ¯ï¼ˆå¯é€‰æ‹©ä¼ å…¥Cookieï¼‰"""
    try:
        cookie = cookie_req.cookie if cookie_req else DEFAULT_COOKIE
        logger.info(f"å°è¯•èŽ·å–ç”¨æˆ·ä¿¡æ¯ï¼ŒCookieé•¿åº¦: {len(cookie) if cookie else 0}")

        crawler = BiliBiliUserCrawler(cookie)

        user_info = crawler.get_user_info()
        if not user_info:
            logger.warning("èŽ·å–ç”¨æˆ·ä¿¡æ¯å¤±è´¥")
            if cookie_req and cookie_req.cookie:
                raise HTTPException(
                    status_code=401, 
                    detail="è‡ªå®šä¹‰Cookieå·²è¿‡æœŸæˆ–æ— æ•ˆã€‚è¯·é‡æ–°èŽ·å–Cookieï¼š\n1. æ‰“å¼€Bç«™å¹¶ç™»å½•\n2. æŒ‰F12æ‰“å¼€å¼€å‘è€…å·¥å…·\n3. åˆ·æ–°é¡µé¢\n4. åœ¨Networkæ ‡ç­¾ä¸­æ‰¾åˆ°ä»»æ„è¯·æ±‚\n5. å¤åˆ¶å®Œæ•´çš„Cookieå€¼"
                )
            else:
                raise HTTPException(
                    status_code=401, 
                    detail="é»˜è®¤Cookieå·²è¿‡æœŸã€‚è¯·è®¾ç½®è‡ªå®šä¹‰Cookieï¼š\n1. æ‰“å¼€Bç«™å¹¶ç™»å½•\n2. æŒ‰F12æ‰“å¼€å¼€å‘è€…å·¥å…·\n3. åˆ·æ–°é¡µé¢\n4. åœ¨Networkæ ‡ç­¾ä¸­æ‰¾åˆ°ä»»æ„è¯·æ±‚\n5. å¤åˆ¶å®Œæ•´çš„Cookieå€¼"
                )

        logger.info(f"æˆåŠŸèŽ·å–ç”¨æˆ·ä¿¡æ¯: {user_info.get('name', 'Unknown')}")
        return user_info
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"èŽ·å–ç”¨æˆ·ä¿¡æ¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}")

@app.post("/api/user/history")
async def get_user_history(cookie_req: CookieRequest = None):
    """èŽ·å–ç”¨æˆ·è§‚çœ‹åŽ†å²"""
    try:
        cookie = cookie_req.cookie if cookie_req else DEFAULT_COOKIE
        crawler = BiliBiliUserCrawler(cookie)

        if not crawler.check_cookie_validity():
            raise HTTPException(status_code=401, detail="Cookieå·²è¿‡æœŸæˆ–æ— æ•ˆ")

        user_info = crawler.get_user_info()
        if not user_info:
            raise HTTPException(status_code=404, detail="æ— æ³•èŽ·å–ç”¨æˆ·ä¿¡æ¯")

        history = crawler.get_watch_history()

        crawler.save_user_data(str(user_info['mid']), 'watch_history', history)

        return {
            "user_info": user_info,
            "history": history,
            "total_count": len(history)
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/user/favorites")
async def get_user_favorites(cookie_req: CookieRequest = None):
    """èŽ·å–ç”¨æˆ·æ”¶è—"""
    try:
        cookie = cookie_req.cookie if cookie_req else DEFAULT_COOKIE
        crawler = BiliBiliUserCrawler(cookie)

        if not crawler.check_cookie_validity():
            raise HTTPException(status_code=401, detail="Cookieå·²è¿‡æœŸæˆ–æ— æ•ˆ")

        user_info = crawler.get_user_info()
        if not user_info:
            raise HTTPException(status_code=404, detail="æ— æ³•èŽ·å–ç”¨æˆ·ä¿¡æ¯")

        favorites = crawler.get_favorites(user_info['mid'])

        crawler.save_user_data(str(user_info['mid']), 'favorites', favorites)

        total_resources = sum(len(folder.get('resources', [])) for folder in favorites)

        return {
            "user_info": user_info,
            "favorites": favorites,
            "folder_count": len(favorites),
            "total_resources": total_resources
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/user/analysis/{user_mid}")
async def get_user_analysis(user_mid: str):
    """èŽ·å–ç”¨æˆ·æ•°æ®åˆ†æž"""
    try:
        with engine.connect() as conn:
            query = text("""
            SELECT data_content, created_at 
            FROM user_data 
            WHERE user_mid = :user_mid AND data_type = 'watch_history'
            ORDER BY created_at DESC 
            LIMIT 1
            """)
            result = conn.execute(query, {'user_mid': user_mid})
            rows = result.fetchall()
            
            if not rows:
                raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°ç”¨æˆ·æ•°æ®")
            
            # è§£æžåŽ†å²æ•°æ®
            history = json.loads(rows[0][0])
            
            # åˆ†æžè§‚çœ‹åå¥½
            categories = defaultdict(int)
            view_times = []
            
            for item in history:
                # ä¼˜å…ˆä½¿ç”¨tag_nameï¼Œå…¶æ¬¡ä½¿ç”¨tname
                category = item.get('tag_name') or item.get('tname')
                if category:
                    categories[category] += 1
                if 'view_at' in item:
                    view_times.append(datetime.fromtimestamp(item['view_at']))
            
            # åˆ†æžè§‚çœ‹æ—¶é—´åˆ†å¸ƒ
            hour_distribution = defaultdict(int)
            for vt in view_times:
                hour_distribution[vt.hour] += 1
            
            # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼ï¼Œé¿å…tupleåºåˆ—åŒ–é—®é¢˜
            most_active_hours = sorted(hour_distribution.items(), key=lambda x: x[1], reverse=True)[:3]
            most_active_hours_list = [{"hour": hour, "count": count} for hour, count in most_active_hours]
            
            analysis = {
                "total_watched": len(history),
                "category_preferences": dict(sorted(categories.items(), key=lambda x: x[1], reverse=True)[:10]),
                "watch_time_distribution": dict(hour_distribution),
                "most_active_hours": most_active_hours_list
            }
            
            return analysis
            
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ==================== æœºå™¨å­¦ä¹ APIç«¯ç‚¹ ====================

@app.get("/api/ml/recommendations")
async def get_video_recommendations(
    video_bvid: str = None, 
    limit: int = 10,
    current_user: dict = Depends(get_current_user)
):
    """èŽ·å–è§†é¢‘æŽ¨è"""
    try:
        with engine.connect() as conn:
            videos_df = pd.read_sql("""
            SELECT bvid, title, view, `like`, coin, share, tname, pubdate, `desc`
            FROM videos 
            ORDER BY collected_at DESC 
            LIMIT 200
            """, conn)
        
        if videos_df.empty:
            raise HTTPException(status_code=404, detail="æš‚æ— è§†é¢‘æ•°æ®")
        
        user_history = None
        recommendation_type = "popular"
        
        # å¦‚æžœç”¨æˆ·å·²ç™»å½•ï¼ŒèŽ·å–ç”¨æˆ·åŽ†å²
        if current_user:
            with engine.connect() as conn:
                query = text("""
                SELECT data_content 
                FROM user_data 
                WHERE user_mid = :user_mid AND data_type = 'watch_history'
                ORDER BY created_at DESC 
                LIMIT 1
                """)
                result = conn.execute(query, {'user_mid': str(current_user['user_id'])})
                rows = result.fetchall()
                
                if rows:
                    user_history = json.loads(rows[0][0])
                    recommendation_type = "collaborative_filtering"
        
        if video_bvid:
            recommendation_type = "content_based"
        
        recommendations = ml_service.get_video_recommendations(
            user_history=user_history,
            video_bvid=video_bvid,
            videos_df=videos_df,
            top_n=limit
        )
        
        return {
            "recommendations": recommendations,
            "total_count": len(recommendations),
            "recommendation_type": recommendation_type,
            "user_logged_in": current_user is not None
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/ml/train-prediction-model")
async def train_prediction_model():
    """è®­ç»ƒæ’­æ”¾é‡é¢„æµ‹æ¨¡åž‹"""
    try:
        with engine.connect() as conn:
            videos_df = pd.read_sql("""
            SELECT bvid, title, view, `like`, coin, share, tname, pubdate, duration
            FROM videos 
            WHERE view > 0
            ORDER BY collected_at DESC 
            LIMIT 1000
            """, conn)
        
        if len(videos_df) < 50:
            raise HTTPException(status_code=400, detail="æ•°æ®é‡ä¸è¶³ï¼Œè‡³å°‘éœ€è¦50ä¸ªè§†é¢‘æ•°æ®")
        
        results = ml_service.train_view_prediction_model(videos_df)
        
        return {
            "message": "æ¨¡åž‹è®­ç»ƒå®Œæˆ",
            "model_performance": results,
            "training_data_size": len(videos_df)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/ml/predict-views")
async def predict_video_views(video_features: dict):
    """é¢„æµ‹è§†é¢‘æ’­æ”¾é‡"""
    try:
        prediction = ml_service.predict_video_views(video_features)

        if prediction is None:
            raise HTTPException(status_code=400, detail="æ¨¡åž‹æœªè®­ç»ƒæˆ–é¢„æµ‹å¤±è´¥")

        return {
            "predicted_views": prediction,
            "input_features": video_features
        }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/ml/user-clustering")
async def analyze_user_clustering():
    """ç”¨æˆ·èšç±»åˆ†æž"""
    try:
        real_users = []

        with engine.connect() as conn:
            users_query = text("""
            SELECT DISTINCT u.id, u.username, u.bilibili_mid, u.bilibili_name
            FROM users u
            WHERE u.bilibili_mid IS NOT NULL
            """)
            users_result = conn.execute(users_query)
            users = users_result.fetchall()
            
            for user in users:
                user_id, username, bilibili_mid, bilibili_name = user
                
                # èŽ·å–ç”¨æˆ·çš„è§‚çœ‹åŽ†å²
                history_query = text("""
                SELECT data_content 
                FROM user_data 
                WHERE user_mid = :user_mid AND data_type = 'watch_history'
                ORDER BY created_at DESC 
                LIMIT 1
                """)
                history_result = conn.execute(history_query, {'user_mid': str(user_id)})
                history_rows = history_result.fetchall()
                
                watch_history = []
                if history_rows:
                    watch_history = json.loads(history_rows[0][0])
                
                # èŽ·å–ç”¨æˆ·åŸºæœ¬ä¿¡æ¯
                info_query = text("""
                SELECT data_content 
                FROM user_data 
                WHERE user_mid = :user_mid AND data_type = 'user_info'
                ORDER BY created_at DESC 
                LIMIT 1
                """)
                info_result = conn.execute(info_query, {'user_mid': str(user_id)})
                info_rows = info_result.fetchall()
                
                user_info = {}
                if info_rows:
                    user_info = json.loads(info_rows[0][0])
                
                real_users.append({
                    'user_info': {
                        'user_id': user_id,
                        'username': username,
                        'bilibili_mid': bilibili_mid,
                        'bilibili_name': bilibili_name,
                        **user_info
                    },
                    'watch_history': watch_history
                })
        
        users_data = real_users
        
        # å¦‚æžœç”¨æˆ·æ•°æ®ä¸è¶³ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
        if len(users_data) < 5:
            # èŽ·å–ä¸€äº›è§†é¢‘æ•°æ®ç”¨äºŽç”Ÿæˆæ¨¡æ‹ŸåŽ†å²
            with engine.connect() as conn:
                videos_df = pd.read_sql("""
                SELECT bvid, title, tname, view, `like`, coin, share, duration
                FROM videos 
                ORDER BY collected_at DESC 
                LIMIT 50
                """, conn)
            
            if not videos_df.empty:
                # ç”Ÿæˆ5ä¸ªæ¨¡æ‹Ÿç”¨æˆ·
                import random
                categories = videos_df['tname'].unique().tolist()
                
                for i in range(5):
                    user_mid = f"mock_user_{i+1}"
                    
                    # ä¸ºæ¯ä¸ªç”¨æˆ·ç”Ÿæˆä¸åŒçš„è§‚çœ‹åå¥½
                    if i == 0:  # é‡åº¦ç”¨æˆ·ï¼Œå–œæ¬¢ç§‘æŠ€
                        preferred_cats = ['ç§‘æŠ€', 'æ•°ç ']
                        watch_count = random.randint(80, 120)
                    elif i == 1:  # å¨±ä¹ç”¨æˆ·
                        preferred_cats = ['å¨±ä¹', 'éŸ³ä¹']
                        watch_count = random.randint(40, 60)
                    elif i == 2:  # æ¸¸æˆç”¨æˆ·
                        preferred_cats = ['æ¸¸æˆ', 'ç”µç«ž']
                        watch_count = random.randint(60, 80)
                    elif i == 3:  # å­¦ä¹ ç”¨æˆ·
                        preferred_cats = ['çŸ¥è¯†', 'æ•™è‚²']
                        watch_count = random.randint(30, 50)
                    else:  # ç»¼åˆç”¨æˆ·
                        preferred_cats = categories[:3]
                        watch_count = random.randint(20, 40)
                    
                    # ç”Ÿæˆè§‚çœ‹åŽ†å²
                    watch_history = []
                    for _ in range(watch_count):
                        # 70%æ¦‚çŽ‡é€‰æ‹©åå¥½åˆ†åŒºçš„è§†é¢‘
                        if random.random() < 0.7 and preferred_cats:
                            cat_videos = videos_df[videos_df['tname'].isin(preferred_cats)]
                            if not cat_videos.empty:
                                video = cat_videos.sample(1).iloc[0]
                            else:
                                video = videos_df.sample(1).iloc[0]
                        else:
                            video = videos_df.sample(1).iloc[0]
                        
                        watch_history.append({
                            'bvid': video['bvid'],
                            'title': video['title'],
                            'tname': video['tname'],
                            'duration': video.get('duration', 300),
                            'view_at': int(time.time()) - random.randint(0, 30*24*3600),  # æœ€è¿‘30å¤©
                            'like': random.randint(0, int(video.get('like', 0) * 0.1)),
                            'coin': random.randint(0, int(video.get('coin', 0) * 0.1)),
                            'share': random.randint(0, int(video.get('share', 0) * 0.1))
                        })
                    
                    users_data.append({
                        'user_mid': user_mid,
                        'user_info': {'mid': user_mid},
                        'watch_history': watch_history
                    })
        
        if len(users_data) < 5:
            raise HTTPException(status_code=400, detail="æ— æ³•ç”Ÿæˆè¶³å¤Ÿçš„ç”¨æˆ·æ•°æ®è¿›è¡Œèšç±»åˆ†æž")
        
        cluster_analysis = ml_service.analyze_user_clusters(users_data)
        
        # è®¡ç®—çœŸå®žç”¨æˆ·æ•°é‡
        real_users_count = len(real_users)
        simulated_users_count = len(users_data) - real_users_count
        
        if simulated_users_count > 0:
            note = f"åŸºäºŽ {real_users_count} ä¸ªçœŸå®žç”¨æˆ·å’Œ {simulated_users_count} ä¸ªæ¨¡æ‹Ÿç”¨æˆ·çš„èšç±»åˆ†æž"
        else:
            note = f"åŸºäºŽ {real_users_count} ä¸ªçœŸå®žç”¨æˆ·çš„èšç±»åˆ†æž"
        
        return {
            "cluster_analysis": cluster_analysis,
            "total_users": len(users_data),
            "real_users_count": real_users_count,
            "simulated_users_count": simulated_users_count,
            "note": note
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/ml/sentiment-analysis")
async def analyze_sentiment(texts: List[str]):
    """æƒ…æ„Ÿåˆ†æž"""
    try:
        if not texts:
            raise HTTPException(status_code=400, detail="æ–‡æœ¬åˆ—è¡¨ä¸èƒ½ä¸ºç©º")

        sentiment_analysis = ml_service.analyze_sentiment(texts)

        return {
            "sentiment_analysis": sentiment_analysis,
            "analyzed_texts_count": len(texts)
        }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/ml/trend-prediction")
async def predict_trends(time_series_data: List[dict], periods: int = 7):
    """è¶‹åŠ¿é¢„æµ‹"""
    try:
        if not time_series_data:
            raise HTTPException(status_code=400, detail="æ—¶é—´åºåˆ—æ•°æ®ä¸èƒ½ä¸ºç©º")

        predictions = ml_service.predict_trends(time_series_data, periods)

        return {
            "predictions": predictions,
            "prediction_periods": periods,
            "input_data_points": len(time_series_data)
        }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/ml/model-status")
async def get_model_status():
    """èŽ·å–æœºå™¨å­¦ä¹ æ¨¡åž‹çŠ¶æ€"""
    try:
        status = {
            "recommendation_system": {
                "initialized": ml_service.recommendation_system is not None,
                "content_features_ready": ml_service.recommendation_system.content_similarity_matrix is not None
            },
            "view_predictor": {
                "initialized": ml_service.view_predictor is not None,
                "model_trained": ml_service.view_predictor.best_model is not None,
                "best_model": getattr(ml_service.view_predictor, 'best_model_name', None),
                "feature_importance": ml_service.view_predictor.feature_importance
            },
            "user_clustering": {
                "initialized": ml_service.user_clustering is not None,
                "n_clusters": ml_service.user_clustering.n_clusters
            },
            "sentiment_analyzer": {
                "initialized": ml_service.sentiment_analyzer is not None
            },
            "trend_predictor": {
                "initialized": ml_service.trend_predictor is not None
            }
        }

        return status

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/ml/similar-users")
async def find_similar_users(current_user: dict = Depends(require_auth)):
    """æ‰¾åˆ°ç›¸ä¼¼ç”¨æˆ·"""
    try:
        users_data = []

        with engine.connect() as conn:
            users_query = text("""
            SELECT DISTINCT u.id, u.username, u.bilibili_mid, u.bilibili_name
            FROM users u
            WHERE u.bilibili_mid IS NOT NULL
            """)
            users_result = conn.execute(users_query)
            users = users_result.fetchall()
            
            for user in users:
                user_id, username, bilibili_mid, bilibili_name = user
                
                # èŽ·å–ç”¨æˆ·çš„è§‚çœ‹åŽ†å²
                history_query = text("""
                SELECT data_content 
                FROM user_data 
                WHERE user_mid = :user_mid AND data_type = 'watch_history'
                ORDER BY created_at DESC 
                LIMIT 1
                """)
                history_result = conn.execute(history_query, {'user_mid': str(user_id)})
                history_rows = history_result.fetchall()
                
                watch_history = []
                if history_rows:
                    watch_history = json.loads(history_rows[0][0])
                
                users_data.append({
                    'user_info': {
                        'user_id': user_id,
                        'username': username,
                        'bilibili_mid': bilibili_mid,
                        'bilibili_name': bilibili_name
                    },
                    'watch_history': watch_history
                })
        
        if len(users_data) < 2:
            return {
                "similar_users": [],
                "message": "ç”¨æˆ·æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œç›¸ä¼¼åº¦åˆ†æž"
            }
        
        # æ‰¾åˆ°ç›¸ä¼¼ç”¨æˆ·
        similar_users = ml_service.find_similar_users(
            target_user_id=current_user['user_id'],
            users_data=users_data,
            top_n=5
        )
        
        return {
            "similar_users": similar_users,
            "total_users": len(users_data),
            "current_user_id": current_user['user_id']
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/ml/user-based-recommendations")
async def get_user_based_recommendations(
    limit: int = 10,
    current_user: dict = Depends(require_auth)
):
    """åŸºäºŽç›¸ä¼¼ç”¨æˆ·çš„æŽ¨è"""
    try:
        with engine.connect() as conn:
            videos_df = pd.read_sql("""
            SELECT bvid, title, view, `like`, coin, share, tname, pubdate, `desc`
            FROM videos 
            ORDER BY collected_at DESC 
            LIMIT 500
            """, conn)
        
        if videos_df.empty:
            raise HTTPException(status_code=404, detail="æš‚æ— è§†é¢‘æ•°æ®")
        
        # èŽ·å–æ‰€æœ‰ç”¨æˆ·æ•°æ®
        users_data = []
        
        with engine.connect() as conn:
            # èŽ·å–æ‰€æœ‰æœ‰Bç«™æ•°æ®çš„ç”¨æˆ·
            users_query = text("""
            SELECT DISTINCT u.id, u.username, u.bilibili_mid, u.bilibili_name
            FROM users u
            WHERE u.bilibili_mid IS NOT NULL
            """)
            users_result = conn.execute(users_query)
            users = users_result.fetchall()
            
            for user in users:
                user_id, username, bilibili_mid, bilibili_name = user
                
                # èŽ·å–ç”¨æˆ·çš„è§‚çœ‹åŽ†å²
                history_query = text("""
                SELECT data_content 
                FROM user_data 
                WHERE user_mid = :user_mid AND data_type = 'watch_history'
                ORDER BY created_at DESC 
                LIMIT 1
                """)
                history_result = conn.execute(history_query, {'user_mid': str(user_id)})
                history_rows = history_result.fetchall()
                
                watch_history = []
                if history_rows:
                    watch_history = json.loads(history_rows[0][0])
                
                users_data.append({
                    'user_info': {
                        'user_id': user_id,
                        'username': username,
                        'bilibili_mid': bilibili_mid,
                        'bilibili_name': bilibili_name
                    },
                    'watch_history': watch_history
                })
        
        if len(users_data) < 2:
            # å¦‚æžœç”¨æˆ·æ•°æ®ä¸è¶³ï¼Œå›žé€€åˆ°æ™®é€šæŽ¨è
            recommendations = ml_service.get_video_recommendations(
                videos_df=videos_df,
                top_n=limit
            )
            return {
                "recommendations": recommendations,
                "recommendation_type": "popular",
                "message": "ç”¨æˆ·æ•°æ®ä¸è¶³ï¼Œæ˜¾ç¤ºçƒ­é—¨æŽ¨è"
            }
        
        # åŸºäºŽç”¨æˆ·ç›¸ä¼¼åº¦çš„æŽ¨è
        recommendations = ml_service.get_user_based_recommendations(
            target_user_id=current_user['user_id'],
            users_data=users_data,
            videos_df=videos_df,
            top_n=limit
        )
        
        return {
            "recommendations": recommendations,
            "recommendation_type": "user_collaborative_filtering",
            "total_users": len(users_data),
            "current_user_id": current_user['user_id']
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ============== AIæ™ºèƒ½é—®ç­”ç›¸å…³æŽ¥å£ ==============

@app.post("/api/ai/chat", response_model=AIQueryResponse)
async def ai_chat(request: AIQueryRequest):
    """
    AIæ™ºèƒ½é—®ç­”æŽ¥å£

    Args:
        request: åŒ…å«ç”¨æˆ·æŸ¥è¯¢å’Œå¯¹è¯åŽ†å²çš„è¯·æ±‚

    Returns:
        AIQueryResponse: AIå›žç­”ç»“æžœ
    """
    try:
        result = await ai_service.chat(
            user_query=request.query,
            conversation_history=request.conversation_history
        )

        return AIQueryResponse(**result)

    except Exception as e:
        logger.error(f"AIèŠå¤©æœåŠ¡å¤±è´¥: {str(e)}")
        return AIQueryResponse(
            success=False,
            response="æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åŽå†è¯•ã€‚",
            timestamp=datetime.now().isoformat(),
            error=str(e)
        )

@app.get("/api/ai/suggestions")
async def get_ai_suggestions():
    """
    èŽ·å–æ™ºèƒ½é—®é¢˜å»ºè®®

    Returns:
        Dict: åŒ…å«å»ºè®®é—®é¢˜åˆ—è¡¨
    """
    try:
        suggestions = ai_service.get_smart_suggestions("")

        return {
            "suggestions": suggestions,
            "timestamp": datetime.now().isoformat()
        }

    except Exception as e:
        logger.error(f"èŽ·å–AIå»ºè®®å¤±è´¥: {str(e)}")
        return {
            "suggestions": [
                "æœ€è¿‘ä¸€å‘¨å“ªä¸ªåˆ†åŒºçš„è§†é¢‘è¡¨çŽ°æœ€å¥½ï¼Ÿ",
                "æ’­æ”¾é‡è¶‹åŠ¿å¦‚ä½•å˜åŒ–ï¼Ÿ",
                "ä»€ä¹ˆç±»åž‹çš„è§†é¢‘æ›´å®¹æ˜“ç«çˆ†ï¼Ÿ",
                "æœ€ä½³å‘å¸ƒæ—¶é—´æ˜¯ä»€ä¹ˆæ—¶å€™ï¼Ÿ"
            ],
            "timestamp": datetime.now().isoformat(),
            "error": str(e)
        }

@app.post("/api/ai/analyze-trend")
async def ai_analyze_trend(metric: str = "view", time_range: str = "7d"):
    """
    AIæ•°æ®è¶‹åŠ¿åˆ†æž

    Args:
        metric: åˆ†æžæŒ‡æ ‡ (view, like, coin, shareç­‰)
        time_range: æ—¶é—´èŒƒå›´

    Returns:
        Dict: è¶‹åŠ¿åˆ†æžç»“æžœ
    """
    try:
        result = await ai_service.analyze_data_trend(
            metric=metric,
            time_range=time_range
        )

        return result

    except Exception as e:
        logger.error(f"AIè¶‹åŠ¿åˆ†æžå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/ai/status")
async def get_ai_status():
    """
    èŽ·å–AIæœåŠ¡çŠ¶æ€

    Returns:
        Dict: AIæœåŠ¡çŠ¶æ€ä¿¡æ¯
    """
    try:
        test_query = "æµ‹è¯•è¿žæŽ¥"
        test_result = await ai_service.chat(test_query, [])

        status = {
            "service_available": test_result.get("success", False),
            "model": "deepseek-v3",
            "api_endpoint": "https://api.deepseek.com",
            "database_connected": ai_service.engine is not None,
            "last_check": datetime.now().isoformat()
        }

        return status

    except Exception as e:
        logger.error(f"AIçŠ¶æ€æ£€æŸ¥å¤±è´¥: {str(e)}")
        return {
            "service_available": False,
            "model": "deepseek-v3",
            "api_endpoint": "https://api.deepseek.com",
            "database_connected": ai_service.engine is not None,
            "last_check": datetime.now().isoformat(),
            "error": str(e)
        }


@app.post("/api/reports/generate-daily")
async def generate_daily_report(request: DailyReportRequest):
    """ç”Ÿæˆæ—¥æŠ¥"""
    try:
        target_date = None
        if request.target_date:
            target_date = datetime.fromisoformat(request.target_date)

        report = await report_service.generate_daily_report(target_date)

        if report["success"]:
            file_path = report_service.save_report(report)
            report["file_path"] = file_path

            clean_report = report_service._clean_data_for_json(report)

            return JSONResponse(content={
                "success": True,
                "message": "æ—¥æŠ¥ç”ŸæˆæˆåŠŸ",
                "report": clean_report
            })
        else:
            return JSONResponse(
                status_code=500,
                content={
                    "success": False,
                    "message": f"æ—¥æŠ¥ç”Ÿæˆå¤±è´¥: {report.get('error', 'æœªçŸ¥é”™è¯¯')}"
                }
            )

    except Exception as e:
        logger.error(f"ç”Ÿæˆæ—¥æŠ¥APIå¤±è´¥: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={
                "success": False,
                "message": f"æœåŠ¡å™¨é”™è¯¯: {str(e)}"
            }
        )

@app.post("/api/reports/generate-weekly")
async def generate_weekly_report(request: WeeklyReportRequest):
    """ç”Ÿæˆå‘¨æŠ¥"""
    try:
        logger.info(f"æ”¶åˆ°å‘¨æŠ¥ç”Ÿæˆè¯·æ±‚: {request}")
        logger.info(f"è¯·æ±‚å‚æ•° - week_start: {request.week_start}")

        week_start = None
        if request.week_start:
            week_start = datetime.fromisoformat(request.week_start)
            logger.info(f"è§£æžåŽçš„week_start: {week_start}")

        logger.info("å¼€å§‹ç”Ÿæˆå‘¨æŠ¥...")
        report = await report_service.generate_weekly_report(week_start)
        logger.info(f"å‘¨æŠ¥ç”Ÿæˆç»“æžœ: success={report.get('success')}")

        if report["success"]:
            logger.info("ä¿å­˜æŠ¥å‘Š...")
            file_path = report_service.save_report(report)
            report["file_path"] = file_path
            logger.info(f"æŠ¥å‘Šä¿å­˜å®Œæˆ: {file_path}")

            logger.info("æ¸…ç†æ•°æ®è¿›è¡ŒJSONåºåˆ—åŒ–...")
            try:
                clean_report = report_service._clean_data_for_json(report)
                logger.info("æ•°æ®æ¸…ç†å®Œæˆ")
            except Exception as clean_error:
                logger.error(f"æ•°æ®æ¸…ç†å¤±è´¥: {str(clean_error)}")
                raise clean_error

            logger.info("æž„å»ºå“åº”...")
            response_data = {
                "success": True,
                "message": "å‘¨æŠ¥ç”ŸæˆæˆåŠŸ",
                "report": clean_report
            }

            try:
                import json
                json.dumps(response_data)
                logger.info("JSONåºåˆ—åŒ–æµ‹è¯•æˆåŠŸ")
            except Exception as json_error:
                logger.error(f"JSONåºåˆ—åŒ–æµ‹è¯•å¤±è´¥: {str(json_error)}")
                raise json_error

            return JSONResponse(content=response_data)
        else:
            logger.error(f"å‘¨æŠ¥ç”Ÿæˆå¤±è´¥: {report.get('error', 'æœªçŸ¥é”™è¯¯')}")
            return JSONResponse(
                status_code=500,
                content={
                    "success": False,
                    "message": f"å‘¨æŠ¥ç”Ÿæˆå¤±è´¥: {report.get('error', 'æœªçŸ¥é”™è¯¯')}"
                }
            )

    except Exception as e:
        logger.error(f"ç”Ÿæˆå‘¨æŠ¥APIå¤±è´¥: {str(e)}")
        logger.error(f"é”™è¯¯ç±»åž‹: {type(e).__name__}")
        import traceback
        logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        return JSONResponse(
            status_code=500,
            content={
                "success": False,
                "message": f"æœåŠ¡å™¨é”™è¯¯: {str(e)}"
            }
        )

@app.get("/api/reports/list")
async def list_reports():
    """èŽ·å–æŠ¥å‘Šåˆ—è¡¨"""
    try:
        import os
        from pathlib import Path

        reports_dir = Path("reports")
        if not reports_dir.exists():
            return JSONResponse(content={
                "success": True,
                "reports": []
            })

        reports = []
        for file_path in reports_dir.glob("*.md"):
            stat = file_path.stat()
            reports.append({
                "filename": file_path.name,
                "path": str(file_path),
                "size": stat.st_size,
                "created_at": datetime.fromtimestamp(stat.st_ctime).isoformat(),
                "modified_at": datetime.fromtimestamp(stat.st_mtime).isoformat()
            })

        reports.sort(key=lambda x: x["modified_at"], reverse=True)

        return JSONResponse(content={
            "success": True,
            "reports": reports
        })

    except Exception as e:
        logger.error(f"èŽ·å–æŠ¥å‘Šåˆ—è¡¨å¤±è´¥: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={
                "success": False,
                "message": f"æœåŠ¡å™¨é”™è¯¯: {str(e)}"
            }
        )

@app.get("/api/reports/download/{filename}")
async def download_report(filename: str):
    """ä¸‹è½½æŠ¥å‘Šæ–‡ä»¶"""
    try:
        from pathlib import Path

        file_path = Path("reports") / filename

        if not file_path.exists():
            return JSONResponse(
                status_code=404,
                content={
                    "success": False,
                    "message": "æŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨"
                }
            )

        return FileResponse(
            path=str(file_path),
            filename=filename,
            media_type='text/markdown'
        )

    except Exception as e:
        logger.error(f"ä¸‹è½½æŠ¥å‘Šå¤±è´¥: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={
                "success": False,
                "message": f"æœåŠ¡å™¨é”™è¯¯: {str(e)}"
            }
        )

@app.get("/api/reports/view/{filename}")
async def view_report(filename: str):
    """æŸ¥çœ‹æŠ¥å‘Šå†…å®¹"""
    try:
        from pathlib import Path

        file_path = Path("reports") / filename

        if not file_path.exists():
            return JSONResponse(
                status_code=404,
                content={
                    "success": False,
                    "message": "æŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨"
                }
            )

        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        return JSONResponse(content={
            "success": True,
            "filename": filename,
            "content": content
        })

    except Exception as e:
        logger.error(f"æŸ¥çœ‹æŠ¥å‘Šå¤±è´¥: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={
                "success": False,
                "message": f"æœåŠ¡å™¨é”™è¯¯: {str(e)}"
            }
        )

@app.delete("/api/reports/{filename}")
async def delete_report(filename: str):
    """åˆ é™¤æŠ¥å‘Šæ–‡ä»¶"""
    try:
        from pathlib import Path

        file_path = Path("reports") / filename

        if not file_path.exists():
            return JSONResponse(
                status_code=404,
                content={
                    "success": False,
                    "message": "æŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨"
                }
            )

        file_path.unlink()

        return JSONResponse(content={
            "success": True,
            "message": "æŠ¥å‘Šåˆ é™¤æˆåŠŸ"
        })

    except Exception as e:
        logger.error(f"åˆ é™¤æŠ¥å‘Šå¤±è´¥: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={
                "success": False,
                "message": f"æœåŠ¡å™¨é”™è¯¯: {str(e)}"
            }
        )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000) 